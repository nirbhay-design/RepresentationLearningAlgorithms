environment: 
YAML: configs/dial.jsd.c10.yaml
==> SEED: 42
==> num_classes: 10
==> data_name: cifar10
==> data_params: {'algo': 'dial', 'data_dir': 'datasets/cifar10', 'batch_size': 512, 'num_workers': 3, 'image_size': 32}
==> return_logs: False
==> eval_every: 10
==> n_epochs: 500
==> n_epochs_mlp: 100
==> gpu_id: 1
==> opt: SGD
==> opt_params: {'lr': 0.5, 'momentum': 0.9, 'nesterov': True}
==> schedular_params: {'T_max': 500, 'eta_min': 0.001}
==> mlp_opt: SGD
==> mlp_opt_params: {'lr': 0.01, 'momentum': 0.9, 'nesterov': True}
==> model_params: {'model_name': 'resnet18', 'pretrained': False, 'proj_dim': 128}
==> mlp_type: hidden
==> loss: dial
==> loss_params: {'sim_type': 'JSD', 'lambd': 0.9}
==> distributed: False
==> train_algo: dial
--------------------------------------------------
===> using hiddin mlp
using optimizer: SGD
using optimizer: SGD
loss function: dial
JSDCL()
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
# of Training Images: 50000
# of Testing Images: 10000
### DiAl Training begins
[GPU1] epochs: [1/500] train_loss_con: -0.122
[GPU1] epochs: [2/500] train_loss_con: -0.166
[GPU1] epochs: [3/500] train_loss_con: -0.182
[GPU1] epochs: [4/500] train_loss_con: -0.188
[GPU1] epochs: [5/500] train_loss_con: -0.201
[GPU1] epochs: [6/500] train_loss_con: -0.204
[GPU1] epochs: [7/500] train_loss_con: -0.210
[GPU1] epochs: [8/500] train_loss_con: -0.214
[GPU1] epochs: [9/500] train_loss_con: -0.217
[GPU1] epochs: [10/500] train_loss_con: -0.219
[GPU1] epochs: [11/500] train_loss_con: -0.224
[GPU1] epochs: [12/500] train_loss_con: -0.227
[GPU1] epochs: [13/500] train_loss_con: -0.229
[GPU1] epochs: [14/500] train_loss_con: -0.230
[GPU1] epochs: [15/500] train_loss_con: -0.236
[GPU1] epochs: [16/500] train_loss_con: -0.235
[GPU1] epochs: [17/500] train_loss_con: -0.238
[GPU1] epochs: [18/500] train_loss_con: -0.241
